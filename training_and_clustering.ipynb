{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as LA\n",
    "import yaml\n",
    "config = yaml.load(open(\"config.yml\"), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_function = lambda a, b: np.round(np.inner(a, b) / (LA.norm(a) * LA.norm(b)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_with_siamese_bert(model, sentences):\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_PCA(embeddings):\n",
    "    t1 = time()\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "    pca = PCA(n_components=3)\n",
    "    embeddings_PCA = pca.fit_transform(scaled_embeddings)\n",
    "    print(\"Final shape : \",embeddings_PCA.shape)\n",
    "    print(\"time : \",(time() - t1))\n",
    "    return embeddings_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_mle(embeddings):\n",
    "    if len(embeddings) < len(embeddings[0]):\n",
    "        return None\n",
    "\n",
    "    t1 = time()\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "    pca = decomposition.PCA(n_components='mle')\n",
    "    x_red = pca.fit_transform(scaled_embeddings)\n",
    "    print(\"PCA mle shape : \", x_red.shape)\n",
    "    print(\"time : \",(time() - t1))\n",
    "    return x_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tars_classifier(train_utterance_tags, config):\n",
    "    data = pd.DataFrame.from_dict({'v1' : list(train_utterance_tags.keys()), 'v2' : list(train_utterance_tags.values())})\n",
    "    data = data[['v1', 'v2']].rename(columns={'v1':'text', 'v2':'label'})\n",
    "    data['label'] = '__label__' + data['label'].astype(str)\n",
    "\n",
    "    data.iloc[0:int(len(data) * 0.8)].to_csv(\"./data_files/tars/\" + 'train.csv', sep='\\t', index=False, header=False)\n",
    "    data.iloc[int(len(data) * 0.8):int(len(data) * 0.9)].to_csv(\"./data_files/tars/\" + 'test.csv', sep='\\t', index=False, header=False)\n",
    "    data.iloc[int(len(data) * 0.9):].to_csv(\"./data_files/tars/\" + 'dev.csv', sep='\\t', index=False, header=False)\n",
    "\n",
    "    column_name_map = {0: \"text\", 1: \"label\"}\n",
    "    train_corpus: Corpus = CSVClassificationCorpus(\"./data_files/tars/\",\n",
    "                                             column_name_map,\n",
    "                                             skip_header=True,\n",
    "                                             delimiter='\\t')\n",
    "\n",
    "    label_dictionary = train_corpus.make_label_dictionary()\n",
    "\n",
    "    tars = TARSClassifier.load('tars-base')\n",
    "    tars.add_and_switch_to_new_task(\"dim\", label_dictionary=label_dictionary)\n",
    "    trainer = ModelTrainer(tars, train_corpus)\n",
    "\n",
    "    trainer.train(base_path=\"./data_files/tars/\", # path to store the model artifacts\n",
    "                  learning_rate=0.02, # use very small learning rate\n",
    "                  mini_batch_size=1, # small mini-batch size since corpus is tiny\n",
    "                  max_epochs=20, # terminate after 20 epochs\n",
    "                  train_with_dev=False,\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_utterances(utterances_to_labels, tars_train_dataset, embed_model, config):\n",
    "    utterances = list(utterances_to_labels.keys())\n",
    "    labels = list(utterances_to_labels.values())\n",
    "    train_tars_classifier(tars_train_dataset, config)\n",
    "\n",
    "    snippet_embeddings_array = []\n",
    "    batch_size = config['siambert_batch_size']\n",
    "    snippet_batches = [utterances[i * batch_size:(i + 1) * batch_size] for i in\n",
    "                       range((len(utterances) + batch_size - 1) // batch_size)]\n",
    "\n",
    "    for snippet_batch in snippet_batches:\n",
    "        snippet_embeddings_batch = embed_with_siamese_bert(embed_model, snippet_batch)\n",
    "        snippet_embeddings_array.extend(snippet_embeddings_batch)\n",
    "    projection_embeddings = reduce_dim(snippet_embeddings_array, config[\"dim_reduction_algo\"])\n",
    "\n",
    "    ids2embeds, ids2prems, ids2projection_embeds, prems2ids, tars_ids2labels = {}, {}, {}, {}, {}\n",
    "    for idx, embed in enumerate(snippet_embeddings_array):\n",
    "        ids2embeds[idx+1] = embed\n",
    "        ids2projection_embeds[idx+1] = projection_embeddings[idx]\n",
    "        ids2prems[idx+1] = utterances[idx]\n",
    "        prems2ids[utterances[idx]] = idx + 1\n",
    "        tars_ids2labels[idx+1] = labels[idx]\n",
    "\n",
    "    with open('./data_files/misc/ids2embeds.pickle', 'wb') as handle:\n",
    "        pickle.dump(ids2embeds, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open('./data_files/misc/ids2projection_embeds.pickle', 'wb') as handle:\n",
    "        pickle.dump(ids2projection_embeds, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    with open('./data_files/misc/ids2prems.pickle', 'wb') as handle:\n",
    "        pickle.dump(ids2prems, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open('./data_files/misc/prems2ids.pickle', 'wb') as handle:\n",
    "        pickle.dump(prems2ids, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open('./data_files/misc/ids2labels.pickle', 'wb') as handle:\n",
    "        pickle.dump(tars_ids2labels, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return ids2prems, ids2embeds, ids2projection_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_unsupervised(ids2embeds):\n",
    "    clusters = {}\n",
    "    premise_ids = list(ids2embeds.keys())\n",
    "    x_red = list(ids2embeds.values())\n",
    "\n",
    "    gmm = GaussianMixture(n_components=66, covariance_type='full')\n",
    "    gmm.fit(x_red)\n",
    "    labels = gmm.predict(x_red)\n",
    "    metrics.silhouette_score(x_red, labels, metric='euclidean')\n",
    "\n",
    "    all_cluster_labels, all_cluster_centers = [], []\n",
    "    cluster_label_centers = {}\n",
    "    centers = np.empty(shape=(gmm.n_components, x_red[0].shape[0]))\n",
    "    for i in range(gmm.n_components):\n",
    "        density = scipy.stats.multivariate_normal(mean=gmm.means_[i], cov=gmm.covariances_[i]).logpdf(x_red)\n",
    "        centers[i, :] = x_red[np.argmax(density)]\n",
    "        all_cluster_labels.append(i)\n",
    "        all_cluster_centers.append(centers[i, :])\n",
    "        cluster_label_centers[i] = centers[i, :]\n",
    "\n",
    "    for i in range(gmm.n_components):\n",
    "        cluster_sample_indices = [idx for idx, label in enumerate(labels) if label == i]\n",
    "        X_samples = []\n",
    "        sample_ids = []\n",
    "        for sample_index in cluster_sample_indices:\n",
    "            X_samples.append(x_red[sample_index])\n",
    "            sample_ids.append(premise_ids[sample_index])\n",
    "\n",
    "        print('X_samples shape : {}\\n'.format(len(X_samples)))\n",
    "\n",
    "        sample_distances = {}\n",
    "        for idx, sample in enumerate(X_samples):\n",
    "            sample_premise_id = sample_ids[idx]\n",
    "            if len(X_samples) < 2:\n",
    "                d = 0\n",
    "            else:\n",
    "                d = (1 - cosine_function(sample.reshape(1, -1), centers[i]))[0]\n",
    "            sample_distances[sample_premise_id] = d\n",
    "        sorted_sample_distances = {k: v for k, v in sorted(sample_distances.items(), key=lambda item: item[1])}\n",
    "        clusters[i] = sorted_sample_distances\n",
    "\n",
    "    with open('./data_files/misc/clusters.pickle', 'wb') as handle:\n",
    "        pickle.dump(clusters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tars_classifier_miscellaneous(config):\n",
    "    model_dir = os.path.join(\"./data_files/misc/\", 'best-model.pt')\n",
    "    tars = TARSClassifier.load(model_dir)\n",
    "    return tars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_dimension_to_clusters_miscellaneous(tars, clusters, ids2prems, threshold):\n",
    "    dimension_tagged_clusters = {}\n",
    "    for label, snippet_score_dict in clusters.items():\n",
    "        snippet_dimension_tags = {}\n",
    "        snippet_ids = []\n",
    "        snippet_texts = []\n",
    "        for snippet_id, dist in list(snippet_score_dict.items()):\n",
    "            if dist > threshold:\n",
    "                break\n",
    "            snippet_ids.append(snippet_id)\n",
    "            snippet_texts.append(ids2prems[snippet_id])\n",
    "\n",
    "        dimensions = flair_classify(snippet_texts, tars)\n",
    "\n",
    "        for i, dimension in enumerate(dimensions):\n",
    "            snippet_id = snippet_ids[i]\n",
    "            snippet_text = snippet_texts[i]\n",
    "            snippet_dimension_tags[snippet_id] = (snippet_text, dimension)\n",
    "        dimension_tagged_clusters[label] = snippet_dimension_tags\n",
    "\n",
    "    tagged_clusters_output = {}\n",
    "    for cluster_label, snippet_dimension_dict in dimension_tagged_clusters.items():\n",
    "        snippet_dimension_tags = []\n",
    "        dimension_frequency = {}\n",
    "        snippet_texts = []\n",
    "        sorted_snippet_dimensions = list(sorted(snippet_dimension_dict.items(), key=lambda x: x[1][1], reverse=True))\n",
    "        for snippet_id, text_to_dimension_tup in sorted_snippet_dimensions:\n",
    "            text = ids2prems[snippet_id].strip()\n",
    "\n",
    "            if text in snippet_texts:\n",
    "                continue\n",
    "\n",
    "            snippet_texts.append(text)\n",
    "            snippet_dimension_tags.append({snippet_id: (text_to_dimension_tup[1][0],\n",
    "                                                                      text_to_dimension_tup[1][1])})\n",
    "\n",
    "            dimension = text_to_dimension_tup[1][0]\n",
    "            score = text_to_dimension_tup[1][1]\n",
    "            if dimension not in dimension_frequency:\n",
    "                dimension_frequency[dimension] = 0\n",
    "            else:\n",
    "                dimension_frequency[dimension] += 1\n",
    "\n",
    "            with open('dimension_tagged_clusters.tsv', 'a') as f:\n",
    "                f.write('{}\\t{}\\t{}\\t{}\\t{}\\n'.format(cluster_label, snippet_id, text_to_dimension_tup[0], text_to_dimension_tup[1][0],\n",
    "                                                  text_to_dimension_tup[1][1]))\n",
    "\n",
    "        tagged_clusters_output[str(cluster_label)] = {'dimension_frequency' : {k : v for k, v in\n",
    "                                                                               sorted(dimension_frequency.items(),\n",
    "                                                                                key= lambda x: x[1], reverse=True)},\n",
    "                                                      'snippet_dimension_tags' : snippet_dimension_tags}\n",
    "\n",
    "    with open('./data_files/misc/tagged_clusters_output.pickle', 'wb') as handle:\n",
    "        pickle.dump(tagged_clusters_output, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return tagged_clusters_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
